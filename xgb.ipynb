{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "from denormalize import denormalize_predictions\n", "from preprocessing import preprocess_consumption_data, read_consumption_data\n", "from train_test_split import (\n", "    split_into_cv_folds_and_test_fold,\n", ")\n", "import xgboost as xgb\n", "from sklearn.metrics import mean_squared_error\n", "import optuna\n", "import json"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["warnings.filterwarnings(\"ignore\", category=FutureWarning)\n", "warnings.filterwarnings(\"ignore\", category=FutureWarning)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_xgb(X_train: pd.DataFrame, y_train: pd.DataFrame, **params):\n", "    \"\"\"\n", "    Trains an XGBoost model.\n", "    \"\"\"\n", "    model = xgb.XGBRegressor(\n", "        verbosity=1, objective=\"reg:squarederror\", booster=\"gbtree\", **params\n", "    )\n", "    model.fit(X_train, y_train)\n", "    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def predict_xgb(model: xgb.XGBRegressor, X_test: pd.DataFrame):\n", "    \"\"\"\n", "    Predicts using an XGBoost model.\n", "    \"\"\"\n", "    return model.predict(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    \"\"\"\n", "    This code is run when executing the file directly.\n", "    \"\"\"\n", "    # %%\n", "    raw_df = read_consumption_data()\n", "    # %%\n", "    # Drop helsingfors\n", "    raw_df = raw_df[raw_df[\"location\"] != \"helsingfors\"]\n\n", "    # %%\n", "    def objective(trial):\n", "        \"\"\"\n", "        Use optuna to select hyperparameters\n", "        \"\"\"\n", "        use_normalization = trial.suggest_categorical(\n", "            \"use_target_normalization\", [True, False]\n", "        )\n", "        use_rolling = False\n", "        if use_normalization:\n", "            use_rolling = False  # trial.suggest_categorical(\"use_rolling_normalization\", [True, False])\n", "        rolling_normalization_window_days = None\n", "        if use_rolling:\n", "            rolling_normalization_window_days = trial.suggest_int(\n", "                \"rolling_normalization_window_days\", 5, 100\n", "            )\n", "        # Based on results, it seemed like num_splits is not an important hyperparam,\n", "        # so we set it to a fixes number instead\n", "        num_splits = 10\n", "        n_estimators = trial.suggest_int(\"n_estimators\", 1, 2000, log=True)\n", "        max_depth = trial.suggest_int(\"max_depth\", 1, 20, log=True)\n", "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-9, 1, log=True)\n", "        # subsample = trial.suggest_float(\"subsample\", 0.1, 1)\n", "        # colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1)\n", "        # gamma = trial.suggest_float(\"gamma\", 0, 1)\n", "        # reg_lambda = trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True)\n\n", "        # %%\n", "        print(\"Preprocessing data...\")\n", "        processed_df = preprocess_consumption_data(\n", "            raw_df, rolling_normalization_window_days\n", "        )\n", "        # %%\n", "        print(\"Splitting\")\n", "        folds = split_into_cv_folds_and_test_fold(\n", "            processed_df,\n", "            n_splits=num_splits,\n", "            target_variable=\"consumption_normalized\"\n", "            if use_normalization\n", "            else \"consumption\",\n", "        )\n", "        # %%\n", "        # Select features\n", "        features = list(folds[0][0][0].columns)\n", "        features_to_use = []\n", "        for feature in features:\n", "            if trial.suggest_int(f\"use_feature_{feature}\", 0, 1):\n", "                features_to_use.append(feature)\n", "        print(json.dumps(trial.params, indent=4))\n", "        if not features_to_use:\n", "            return float(\"inf\"), float(\"inf\")\n", "        # %%\n", "        cv_folds = folds[:-1]\n", "        results_dfs = []\n", "        for i, (training, validation) in enumerate(cv_folds):\n", "            print(f\"CV fold {i}\")\n", "            X_train, y_train = training\n", "            X_val, y_val = validation\n", "            X_train = X_train[features_to_use]\n", "            X_train = pd.get_dummies(X_train)\n", "            model = train_xgb(\n", "                X_train,\n", "                y_train,\n", "                max_depth=max_depth,\n", "                learning_rate=learning_rate,\n", "                n_estimators=n_estimators,\n", "                # subsample=subsample,\n", "                # colsample_bytree=colsample_bytree,\n", "                # gamma=gamma,\n", "                # reg_lambda=reg_lambda,\n", "            )\n", "            X_val = X_val[features_to_use]\n", "            X_val = pd.get_dummies(X_val)\n", "            X_val = X_val.reindex(columns=X_train.columns, fill_value=0)[\n", "                X_train.columns\n", "            ]\n", "            y_val_pred = predict_xgb(model, X_val)\n", "            rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n", "            print(f\"RMSE (normalized): {rmse}\")\n", "            results_df = pd.DataFrame(\n", "                {\n", "                    \"actual\": y_val,\n", "                    \"prediction\": y_val_pred,\n", "                },\n", "                index=y_val.index,\n", "            )\n", "            if use_normalization:\n", "                denormalized_results_df = denormalize_predictions(\n", "                    results_df, raw_df, rolling_normalization_window_days\n", "                )\n", "            else:\n", "                denormalized_results_df = results_df\n", "            denormalized_results_df[\"fold\"] = i\n", "            results_dfs.append(denormalized_results_df)\n\n", "        # %%\n", "        # Merge result dataframes\n", "        results_df = pd.concat(results_dfs)\n\n", "        # %%\n", "        # Calculate RMSE for denormalized data\n", "        rmse = np.sqrt(\n", "            mean_squared_error(results_df[\"actual\"], results_df[\"prediction\"])\n", "        )\n", "        print(f\"RMSE: {rmse}\")\n\n", "        # Calculate mean absolute percentage error for denormalized data\n", "        results_df[\"PE\"] = 100 * (\n", "            (results_df[\"actual\"] - results_df[\"prediction\"]) / results_df[\"actual\"]\n", "        )\n", "        results_df[\"APE\"] = np.abs(results_df[\"PE\"])\n", "        mape = results_df[\"APE\"].mean()\n", "        print(f\"MAPE: {results_df['APE'].mean()}%\")\n", "        print(\n", "            \"MAPE per\",\n", "            results_df.groupby(\n", "                results_df.index.get_level_values(\"location\"), observed=True\n", "            )[\"APE\"].mean(),\n", "        )\n", "        print(\"MAPE per fold\", results_df.groupby(\"fold\")[\"APE\"].mean())\n", "        print(\"\\nMPE per fold\", results_df.groupby(\"fold\")[\"PE\"].mean())\n\n", "        # %%\n", "        # Print info about the fold\n", "        print(\n", "            \"Fold time info:\\n\",\n", "            results_df.reset_index().groupby(\"fold\")[\"time\"].agg([\"min\", \"max\"]),\n", "        )\n\n", "        # %%\n", "        # Look at performance in Oslo\n", "        # for date in results_df.reset_index()[\"time\"].dt.date.unique():\n", "        #     values_on_date = results_df.reset_index().loc[\n", "        #         pd.Series(results_df.index.get_level_values(\"time\")).dt.date == date\n", "        #     ]\n", "        #     values_on_date[values_on_date[\"location\"] == \"oslo\"].set_index(\"time\")[\n", "        #         [\"actual\", \"prediction\"]\n", "        #     ].plot()\n\n", "        # %%\n", "        return rmse, mape\n\n", "    # %%\n", "    sampler = optuna.samplers.NSGAIISampler()\n", "    study_name = f\"XGBoost consumption prediction {sampler.__class__.__name__}\"\n", "    # %%\n", "    try:\n", "        study = optuna.load_study(study_name=study_name, storage=\"sqlite:///optuna.db\")\n", "    except KeyError as e:\n", "        study = optuna.create_study(\n", "            directions=[\"minimize\", \"minimize\"],\n", "            storage=\"sqlite:///optuna.db\",\n", "            study_name=study_name,\n", "            sampler=sampler,\n", "        )\n", "    def one_fold_validation():\n", "        \"\"\"\n", "        Get validation scores for the current set of chosen parameters\n", "        (useful for simple testing)\n", "        \"\"\"\n", "        use_normalization = True\n", "        rolling_normalization_window_days = None\n", "        num_splits = 10\n", "        n_estimators = 50\n", "        max_depth = 8\n", "        learning_rate = 0.03\n", "        processed_df = preprocess_consumption_data(\n", "            raw_df, rolling_normalization_window_days\n", "        )\n", "        folds = split_into_cv_folds_and_test_fold(\n", "            processed_df,\n", "            n_splits=num_splits,\n", "            target_variable=\"consumption_normalized\"\n", "            if use_normalization\n", "            else \"consumption\",\n", "        )\n", "        training, validation = folds[-2]\n", "        X_train, y_train = training\n", "        X_val, y_val = validation\n", "        X_train = pd.get_dummies(X_train)\n", "        model = train_xgb(\n", "            X_train,\n", "            y_train,\n", "            max_depth=max_depth,\n", "            learning_rate=learning_rate,\n", "            n_estimators=n_estimators,\n", "        )\n", "        X_val = pd.get_dummies(X_val)\n", "        X_val = X_val.reindex(columns=X_train.columns, fill_value=0)[X_train.columns]\n", "        y_val_pred = predict_xgb(model, X_val)\n", "        rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n", "        print(f\"RMSE (normalized): {rmse}\")\n", "        results_df = pd.DataFrame(\n", "            {\n", "                \"actual\": y_val,\n", "                \"prediction\": y_val_pred,\n", "            },\n", "            index=y_val.index,\n", "        )\n", "        if use_normalization:\n", "            denormalized_results_df = denormalize_predictions(\n", "                results_df, raw_df, rolling_normalization_window_days\n", "            )\n", "        else:\n", "            denormalized_results_df = results_df\n\n", "        # Calculate RMSE for denormalized data\n", "        rmse = np.sqrt(\n", "            mean_squared_error(\n", "                denormalized_results_df[\"actual\"], denormalized_results_df[\"prediction\"]\n", "            )\n", "        )\n", "        print(f\"RMSE: {rmse}\")\n\n", "        # Calculate mean absolute percentage error for denormalized data\n", "        denormalized_results_df[\"PE\"] = 100 * (\n", "            (denormalized_results_df[\"actual\"] - denormalized_results_df[\"prediction\"])\n", "            / denormalized_results_df[\"actual\"]\n", "        )\n", "        denormalized_results_df[\"APE\"] = np.abs(denormalized_results_df[\"PE\"])\n", "        mape = denormalized_results_df[\"APE\"].mean()\n", "        print(f\"MAPE: {mape}%\")\n\n", "    # one_fold_validation()\n", "    study.optimize(objective, n_trials=100)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}